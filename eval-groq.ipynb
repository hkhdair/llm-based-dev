{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv \n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of low latency LLMs\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low latency Large Language Models (LLMs) are important in certain applications due to their ability to process and respond to inputs quickly. Latency refers to the time delay between a user's request and the system's response. In some real-time or time-sensitive applications, low latency is crucial for providing a good user experience and ensuring that the system can respond to changing conditions in a timely manner.\n",
      "\n",
      "For example, in conversational agents or chatbots, low latency is important for maintaining the illusion of a real-time conversation. If there is a significant delay between the user's input and the agent's response, it can disrupt the flow of the conversation and make it feel less natural. Similarly, in applications such as online gaming or financial trading, low latency is critical for enabling users to make decisions and take actions quickly based on real-time data.\n",
      "\n",
      "Moreover, low latency LLMs can help reduce the computational cost of running large language models. By reducing the amount of time that the model spends processing each input, it is possible to serve more requests within a given amount of time, or to use fewer resources to serve the same number of requests. This can make large language models more practical and cost-effective to deploy in real-world applications.\n",
      "\n",
      "Overall, low latency LLMs are important for enabling real-time or time-sensitive applications to provide a good user experience, make decisions quickly, and reduce computational cost.\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "Title: Inference in Deep Learning Models: A Comprehensive Overview\n",
      "\n",
      "Introduction:\n",
      "\n",
      "Inference in deep learning models refers to the process of using an already trained model to predict new, unseen data. Deep learning models, which are a subset of machine learning models, are designed to automatically and adaptively learn features from data, have gained significant attention in recent years due to their remarkable performance in various applications such as image recognition, natural language processing, and speech recognition. This article aims to provide a comprehensive overview of inference in deep learning models, discussing its importance, approaches, and challenges.\n",
      "\n",
      "Importance of Inference:\n",
      "\n",
      "Inference is a critical component of deep learning models as it enables the practical application of these models in real-world scenarios. After a model has been trained on a dataset, it can be used to make predictions on new, unseen data. This process, known as inference, is where the value of deep learning models is truly realized. For instance, an image recognition model trained to identify objects in images can be used to analyze security footage, enabling real-time detection of potential threats.\n",
      "\n",
      "Approaches to Inference:\n",
      "\n",
      "There are two main approaches to inference in deep learning models:\n",
      "\n",
      "1. **Deterministic Inference**: This approach involves using the trained model to make a single prediction for a given input. The model's parameters, which have been optimized during the training process, are used to compute the output. This method is straightforward and efficient, making it the most common approach for inference.\n",
      "2. **Bayesian Inference**: This approach involves treating the model's parameters as random variables and calculating the posterior distribution over these parameters given the observed data. This method allows for the quantification of uncertainty in the model's predictions, which can be valuable in certain applications. However, it is computationally intensive and often requires approximations.\n",
      "\n",
      "Challenges in Inference:\n",
      "\n",
      "Despite its importance, inference in deep learning models presents several challenges:\n",
      "\n",
      "1. **Computational Complexity**: Deep learning models, particularly those with a large number of layers or parameters, can be computationally expensive to evaluate during inference. This can limit their applicability in real-time or resource-constrained environments.\n",
      "2. **Uncertainty Quantification**: While deep learning models have shown remarkable performance, they are often criticized for their lack of transparency and interpretability. This makes it difficult to quantify the uncertainty in their predictions, which can be crucial in safety-critical applications.\n",
      "3. **Data Efficiency**: Deep learning models typically require large amounts of data for training, which can be a limitation in scenarios where data is scarce or expensive to obtain.\n",
      "\n",
      "Conclusion:\n",
      "\n",
      "Inference in deep learning models is a critical aspect of these models' practical application. While it offers significant benefits, it also presents several challenges that need to be addressed. Future research should focus on developing efficient and robust inference methods that can overcome these challenges, thereby enhancing the usability and reliability of deep learning models in real-world scenarios."
     ]
    }
   ],
   "source": [
    "%time\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a professional academic instructor. Provide your answers in academic style, formatted in paragraphs and bullet points, supported with explanations and examples where possible.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write an article about inference for deep learning models.\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.5,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "    # display(Markdown(chunk.choices[0].delta.content or \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using LangChain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the groq and langchain-groq package if not already installed:\n",
    "`pip install groq langchain-groq`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatGroq(temperature=0, \n",
    "                groq_api_key=getenv(\"GROQ_API_KEY\"), \n",
    "                model_name=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Latency Large Language Models (LLMs) are critical in many applications due to several reasons:\n",
      "\n",
      "1. Real-time interaction: Low latency LLMs can process user inputs quickly, providing real-time interaction, which is essential for applications such as chatbots, voice assistants, and online gaming. Users expect immediate responses, and high latency can lead to a poor user experience.\n",
      "2. Improved user engagement: Low latency LLMs can maintain user engagement by providing quick and accurate responses. High latency can cause users to lose interest, leading to a poor user experience and reduced engagement.\n",
      "3. Enhanced accuracy: Low latency LLMs can improve the accuracy of the generated responses. When LLMs take too long to process user inputs, the context of the conversation can be lost, leading to inaccurate or irrelevant responses.\n",
      "4. Better decision-making: Low latency LLMs can provide real-time insights and recommendations, enabling better decision-making. For instance, in financial trading or healthcare applications, low latency LLMs can help make critical decisions quickly, leading to better outcomes.\n",
      "5. Competitive advantage: Low latency LLMs can provide a competitive advantage in industries where real-time processing and decision-making are critical. For instance, in online gaming or e-commerce, low latency LLMs can help businesses stay ahead of their competitors by providing a faster and more responsive user experience.\n",
      "\n",
      "In summary, low latency LLMs are essential for providing a fast, accurate, and engaging user experience. They can improve decision-making, enhance user engagement, and provide a competitive advantage in various industries.\n"
     ]
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "response = chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGroq also supports `async` and `streaming` functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There once was a star named Sun,\n",
      "Shining bright since time begun.\n",
      "It rises and sets,\n",
      "In fiery reds and golds,\n",
      "Warming all of us, one by one.\n"
     ]
    }
   ],
   "source": [
    "chat = ChatGroq(temperature=0, model_name=\"mixtral-8x7b-32768\")\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", \"Write a Limerick about {topic}\")])\n",
    "chain = prompt | chat\n",
    "response = await chain.ainvoke({\"topic\": \"The Sun\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The moon's gentle glow\n",
      "Illuminates the night sky\n",
      "Peaceful and serene"
     ]
    }
   ],
   "source": [
    "chat = ChatGroq(temperature=0, model_name=\"llama2-70b-4096\")\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", \"Write a haiku about {topic}\")])\n",
    "chain = prompt | chat\n",
    "for chunk in chain.stream({\"topic\": \"The Moon\"}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groq also supports Gemma 7b instruct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Best Practices for Training Large Language Models (LLMs)**\n",
       "\n",
       "**1. Data Collection and Preprocessing:**\n",
       "- Gather a massive, high-quality dataset that is relevant to the task at hand.\n",
       "- Perform data preprocessing steps such as tokenization, padding, and normalization.\n",
       "\n",
       "**2. Model Architecture and Optimization:**\n",
       "- Choose an appropriate LLM architecture, such as Transformer, GPT-3, or BERT.\n",
       "- Fine-tune the model parameters on the task-specific data.\n",
       "- Optimize hyperparameters, such as learning rate, batch size, and number of layers.\n",
       "\n",
       "**3. Training Procedure:**\n",
       "- Use a powerful optimizer, such as Adam or SGD.\n",
       "- Implement gradient clipping and other regularization techniques to prevent overfitting.\n",
       "- Use early stopping to terminate training when the model stops improving.\n",
       "\n",
       "**4. Model Tuning:**\n",
       "- Fine-tune the model on the specific task data to optimize performance.\n",
       "- Use techniques like zero-shot learning or transfer learning to reduce training time.\n",
       "\n",
       "**5. Model Evaluation:**\n",
       "- Evaluate the model's performance on a held-out dataset.\n",
       "- Use metrics such as accuracy, precision, and recall to measure performance.\n",
       "\n",
       "**6. Model Deployment:**\n",
       "- Deploy the trained model into a production environment.\n",
       "- Consider factors such as latency, scalability, and cost.\n",
       "\n",
       "**Additional Tips:**\n",
       "\n",
       "- **Use high-quality hardware:** LLMs require significant computational resources, so using powerful hardware is crucial.\n",
       "- **Utilize distributed training:** Train the model on multiple machines to accelerate training time.\n",
       "- **Seek expert guidance:** Consult with experienced LLM engineers and data scientists for guidance and best practices.\n",
       "- **Stay up-to-date:** Keep up with the latest advancements in LLM training techniques.\n",
       "\n",
       "**Note:** Training LLMs is a complex and resource-intensive process. It requires a large amount of data, computational resources, and time. It is recommended to consult with experts or use cloud-based services to simplify the training process."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = Groq(\n",
    "    api_key=getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the best way to train a Large Language Model?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gemma-7b-it\",\n",
    ")\n",
    "\n",
    "# print(chat_completion.choices[0].message.content)\n",
    "display(Markdown(chat_completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
